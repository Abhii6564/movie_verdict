{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Movie Success Prediction\n",
                "\n",
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "from sklearn.impute import SimpleImputer\n",
                "import warnings\n",
                "import pickle\n",
                "\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('movie_metadata.csv')\n",
                "print(df.shape)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "plt.figure(figsize=(10,6))\n",
                "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
                "plt.title(\"Missing Values Heatmap\")\n",
                "plt.show()\n",
                "\n",
                "print(df.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target Variable Distribution\n",
                "plt.figure(figsize=(8,5))\n",
                "sns.histplot(df['imdb_score'], bins=20, kde=True)\n",
                "plt.title(\"Distribution of IMDB Scores\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation Heatmap (Numerical columns)\n",
                "plt.figure(figsize=(12,10))\n",
                "numeric_df = df.select_dtypes(include=[np.number])\n",
                "sns.heatmap(numeric_df.corr(), annot=False, cmap='coolwarm')\n",
                "plt.title(\"Correlation Heatmap\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Preprocessing & Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Drop duplicates\n",
                "df.drop_duplicates(inplace=True)\n",
                "\n",
                "# 2. Handle Missing Values\n",
                "# Numeric: Median imputation\n",
                "num_cols = df.select_dtypes(include=[np.number]).columns\n",
                "imputer_num = SimpleImputer(strategy='median')\n",
                "df[num_cols] = imputer_num.fit_transform(df[num_cols])\n",
                "\n",
                "# Categorical: Mode imputation (if any needed, though we drop many)\n",
                "cat_cols = df.select_dtypes(include=['object']).columns\n",
                "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
                "df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])\n",
                "\n",
                "# 3. Create Target Variable 'Classify'\n",
                "# [1-3] -> Flop, [3-6] -> Average, [6-10] -> Hit\n",
                "def classify_movie(score):\n",
                "    if score < 3:\n",
                "        return 'Flop'\n",
                "    elif score < 6:\n",
                "        return 'Average'\n",
                "    else:\n",
                "        return 'Hit'\n",
                "\n",
                "df['Classify'] = df['imdb_score'].apply(classify_movie)\n",
                "\n",
                "print(df['Classify'].value_counts())\n",
                "\n",
                "# 4. Feature Selection\n",
                "# User requested specific inputs for app: genre, imdb_score, fb_likes, budget, year\n",
                "# We will train the model using: 'genres', 'cast_total_facebook_likes', 'budget', 'title_year'\n",
                "# Note: We drop 'imdb_score' from X because it is the source of y (Leakage)\n",
                "\n",
                "# Simplify Genre: Take the first genre listed\n",
                "df['main_genre'] = df['genres'].apply(lambda x: x.split('|')[0] if isinstance(x, str) else x)\n",
                "\n",
                "# Select Features\n",
                "features = ['main_genre', 'cast_total_facebook_likes', 'budget', 'title_year']\n",
                "X = df[features]\n",
                "y = df['Classify']\n",
                "\n",
                "# 5. Encoding\n",
                "le_genre = LabelEncoder()\n",
                "X['main_genre'] = le_genre.fit_transform(X['main_genre'])\n",
                "\n",
                "# Save LabelEncoder for App\n",
                "with open('label_encoder_genre.pkl', 'wb') as f:\n",
                "    pickle.dump(le_genre, f)\n",
                "\n",
                "# 6. Scaling (Optional for RF but good for others)\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "# Save Scaler for App\n",
                "with open('scaler.pkl', 'wb') as f:\n",
                "    pickle.dump(scaler, f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Development"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split Data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Initialize Models\n",
                "models = {\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
                "    'Logistic Regression': LogisticRegression(random_state=42)\n",
                "}\n",
                "\n",
                "results = {}\n",
                "\n",
                "# Train and Evaluate\n",
                "for name, model in models.items():\n",
                "    model.fit(X_train, y_train)\n",
                "    y_pred = model.predict(X_test)\n",
                "    accuracy = accuracy_score(y_test, y_pred)\n",
                "    results[name] = accuracy\n",
                "    print(f\"--- {name} ---\")\n",
                "    print(f\"Accuracy: {accuracy:.4f}\")\n",
                "    print(classification_report(y_test, y_pred))\n",
                "    print(\"-\"*30)\n",
                "\n",
                "# Visualize Results\n",
                "plt.figure(figsize=(8,5))\n",
                "plt.bar(results.keys(), results.values(), color=['blue', 'green', 'orange'])\n",
                "plt.title(\"Model Comparison - Accuracy\")\n",
                "plt.ylabel(\"Accuracy\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Final Model Saving"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We'll use Random Forest as the final model for the app\n",
                "final_model = models['Random Forest']\n",
                "with open('model.pkl', 'wb') as f:\n",
                "    pickle.dump(final_model, f)\n",
                "print(\"Model saved as model.pkl\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}